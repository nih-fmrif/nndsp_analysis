{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from datetime import date\n",
    "import re\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "## EDIT THESE VARIABLES\n",
    "analysis_version = \"2017_10_19\"\n",
    "project_dir = Path('/Users/rodgersleejg/data/hpc/NNDSP') # needs to be pathlib.Path object\n",
    "\n",
    "bids_dir = project_dir.joinpath('bids_2017_07_14')\n",
    "\n",
    "# conf_script = mriqc_dir.joinpath('conf' + analysis_version + '.sh')\n",
    "\n",
    "mriqc_dir  = project_dir.joinpath('anal/mriqc_files/other_files')\n",
    "if not mriqc_dir.exists():\n",
    "    mriqc_dir.mkdir()\n",
    "output_folder =  project_dir / 'derivatives' / 'mriqc'\n",
    "if not output_folder.exists():\n",
    "    output_folder.mkdir()\n",
    "classifier_output =  output_folder.joinpath('classifier')\n",
    "if not classifier_output.exists():\n",
    "    classifier_output.mkdir()\n",
    "base_work_dir = output_folder.joinpath('work')\n",
    "if not base_work_dir.exists():\n",
    "    base_work_dir.mkdir()\n",
    "log_dir = mriqc_dir.joinpath('swarm_output_' +  analysis_version)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "manual_qc = output_folder.joinpath('manual_qc_round_2.tsv')\n",
    "# swarm_path = mriqc_dir.joinpath('mriqc_' + analysis_version + '.cmd')\n",
    "\n",
    "mriqc_with_predictions = Path('derivatives/mriqc/with_mriqc_predictions.csv')\n",
    "plottable_data = Path('derivatives/mriqc/classifer_plot_data.pklz')\n",
    "qc_data = Path('anal/mriqc_files/other_files/qc_pickle_for_v2_exploration.pklz')\n",
    "\n",
    "%pwd\n",
    "%cd {project_dir}\n",
    "%pwd\n",
    "\n",
    "import anal.python_modules.inner_merge_and_report as pd_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up to date qc data:\n",
    "print('Running mriqc_exploration.ipynb from mriqc_performance.ipynb:')\n",
    "%run 'anal/mriqc_files/analysis_notebooks/mriqc_exploration.ipynb'\n",
    "# above is dependent on merge_qc_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get performance metrics for the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qc_full = pd.read_pickle(qc_data)\n",
    "df_qc_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anal.python_modules import classification\n",
    "reload(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Pdb; ipdb=Pdb()\n",
    "df_qc_temp = df_qc_full.copy()\n",
    "manual_metrics = ['Freesurfer_avg_ext_rating', 'Freesurfer_avg_int_rating', 'MPRAGE']\n",
    "classifier_metrics = ['tpr','fpr','fdr','fp','tp','fn','tn']\n",
    "for metric in manual_metrics:\n",
    "    col_prob = 'prob_y'\n",
    "    col_true = metric + '_thresholded'\n",
    "    threshold = 3\n",
    "    df_qc_temp[col_true] = df_qc_temp[metric] >= threshold\n",
    "    df_performance = classification.get_classification_scores(df_qc_temp,col_true,col_prob)\n",
    "#     ipdb.runcall(classification.get_classification_scores,df_qc_temp,col_true,col_prob)\n",
    "    df_qc_temp.drop(df_performance.columns,axis = 1, inplace=True,errors='ignore')\n",
    "    df_performance = pd.concat([df_qc_temp, df_performance],axis = 1)\n",
    "    \n",
    "del df_qc_temp\n",
    "df_performance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather value columns together using melt and create labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value cols need to all be the same type so that they can be melted to a single columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cols = df_performance.filter(regex = '^(Free|MP).*(' + 'thresholded' + ')').columns\n",
    "df_performance.loc[:,t_cols] = df_performance.loc[:,t_cols].apply(lambda col:col.astype(float),axis = 0)\n",
    "\n",
    "tail_of_regex = '|'.join(classifier_metrics) + '|thresholded'\n",
    "cols_regex = '^(Free|MP).*(' + tail_of_regex + ')'\n",
    "value_cols = df_performance.filter(regex= cols_regex, axis=1).columns\n",
    "ids_to_keep = pd.Index(['MASKID','run','prob_y', 'pred_y','threshold'])\n",
    "\n",
    "print('Regex for value columns to be melted,separated and pivoted: ', cols_regex)\n",
    "print('ids: ',ids_to_keep,'\\n\\n\\nvalues: ',value_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = df_performance.melt(id_vars = ids_to_keep,\n",
    "                var_name= 'binarized_manual_qc_scores',\n",
    "                            value_name= 'value',\n",
    "                value_vars= value_cols)\n",
    "df_melted = (\n",
    "    pd.concat(\n",
    "        [df_melted,\n",
    "        (df_melted.\n",
    "         binarized_manual_qc_scores.\n",
    "         str.\n",
    "         extract(expand=True,\n",
    "                 pat= '(?P<manual_qc_type>.*)_(?P<value_type>' + tail_of_regex  + ')')\n",
    "        )],\n",
    "    axis = 1)\n",
    ")\n",
    "df_melted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column each for the tpr and fpr variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MASKID','run', 'manual_qc_type','prob_y','pred_y','value_type']\n",
    "df_roc = df_melted[[*cols,'value']].set_index(cols).unstack().reset_index()\n",
    "cols_from_pivot = df_roc.columns.levels[1][:-1]\n",
    "df_roc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roc.columns = [*cols[:-1], *cols_from_pivot]\n",
    "df_roc['fpratio'] = df_roc.fp/df_roc.tp\n",
    "df_roc['positive'] = df_roc.fp + df_roc.tp\n",
    "df_roc.to_pickle(plottable_data)\n",
    "df_roc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_roc.query('manual_qc_type ==\"MPRAGE\"')\n",
    "df_roc.query('manual_qc_type ==\"MPRAGE\"').groupby('thresholded').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( plottable_data, 'Size:',len(df_roc.query('manual_qc_type == \"MPRAGE\"')))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
